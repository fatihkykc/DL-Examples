{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pJbYXou6chZf"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from torchvision import models\n",
    "import pdb\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtQNZ6bAdNc5"
   },
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch\n",
    "!pip install densenet_pytorch\n",
    "!pip install resnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "seREDTC-cEsb"
   },
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "from densenet_pytorch import DenseNet \n",
    "from resnet_pytorch import ResNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rYI-k7cYcEsc"
   },
   "outputs": [],
   "source": [
    "# import os, glob, shutil\n",
    "# for idx, file in enumerate(glob.glob('./MURA-v1.1/valid/XR_WRIST/patient*/*/*')):\n",
    "#     splitted = file.split('/')\n",
    "#     label = [0 if splitted[-2].split('_')[1]==\"negative\" else 1][0]\n",
    "#     new_filename=str(label)+\"_\"+str(idx+17000)+\".png\"\n",
    "#     shutil.copy(file, './muradatavalid/'+new_filename)\n",
    "\n",
    "# import os, glob, shutil\n",
    "# for idx, file in enumerate(glob.glob('./MURA-v1.1/train/XR_WRIST/patient*/*/*')):\n",
    "#     splitted = file.split('/')\n",
    "#     label = [0 if splitted[-2].split('_')[1]==\"negative\" else 1][0]\n",
    "#     new_filename=str(label)+\"_\"+str(idx+17000)+\".png\"\n",
    "#     shutil.copy(file, './Medathon_Mura/'+new_filename)\n",
    "\n",
    "# Bütün resimler \"Medathon_Mura\" klasörüne atıldı\n",
    "# \"muradatavalid\" klasörü, training sürecindeki validasyon resimlerini içeriyor\n",
    "# \"Medathon_Mura\" klasörü hem MURA hem de Medathon verisetlerinin anormal ve normal resimlerinin tümünü içeriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7dliaxqDce_K"
   },
   "outputs": [],
   "source": [
    "training_folder = '/content/drive/MyDrive/Medathon_Mura'\n",
    "valid_folder = '/content/drive/MyDrive/muradatavalid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PEGsYnrNzl-B"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"filename\":os.listdir(training_folder)})\n",
    "df[\"label\"] = df[\"filename\"].apply(lambda x: x[0])\n",
    "df.to_csv('train.csv', index=False)\n",
    "\n",
    "valid_df = pd.DataFrame({\"filename\":os.listdir(valid_folder)})\n",
    "valid_df[\"label\"] = valid_df[\"filename\"].apply(lambda x: int(x[0]))\n",
    "valid_df.to_csv('valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ySctCtkSzvjX"
   },
   "outputs": [],
   "source": [
    "loss_fn = F.binary_cross_entropy_with_logits\n",
    "loss_name = \"BCEWithLogits\"\n",
    "epochs=1\n",
    "height=64\n",
    "width=64\n",
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qXEuwZkQzxru"
   },
   "outputs": [],
   "source": [
    "class MedathonDataset(Dataset):\n",
    "    def __init__(self,df,im_path,transforms=None,is_test=False):\n",
    "        self.df = df\n",
    "        self.im_path = im_path\n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = f\"{self.im_path}/{self.df.iloc[idx]['filename']}\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms:\n",
    "                img = self.transforms(image=np.array(img))[\"image\"]\n",
    "        if self.is_test:\n",
    "            return img\n",
    "        target = self.df.iloc[idx]['label']\n",
    "        return img, torch.tensor([target],dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jPUlhUGIzxp5"
   },
   "outputs": [],
   "source": [
    "def get_train_val_split(df):\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.3)\n",
    "    return train_df,valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2aSpWfm-zxnc"
   },
   "outputs": [],
   "source": [
    "def get_augmentations(p=0.5, height, width):\n",
    "    train_tfms = A.Compose([\n",
    "        A.Resize(height,width),\n",
    "        A.Cutout(num_holes=12, p=p),\n",
    "        A.RandomRotate90(p=p),\n",
    "        A.Flip(p=p),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                       contrast_limit=0.2,\n",
    "                                       ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=20,\n",
    "                sat_shift_limit=50,\n",
    "                val_shift_limit=50)\n",
    "        ], p=p),\n",
    "        A.OneOf([\n",
    "            A.IAAAdditiveGaussianNoise(),\n",
    "            A.GaussNoise(),\n",
    "        ], p=p),\n",
    "        A.OneOf([\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=p),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=p),\n",
    "        A.OneOf([\n",
    "            A.GridDistortion(p=0.1),\n",
    "        ], p=p),\n",
    "        ])\n",
    "    \n",
    "    test_tfms = A.Compose([\n",
    "        A.Resize(height,width),\n",
    "        ])\n",
    "    return train_tfms, test_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3m3eA5nAzxlK"
   },
   "outputs": [],
   "source": [
    "class MedathonEfficientNet(nn.Module):\n",
    "    def __init__(self,model_name='efficientnet-b0',pool_type=F.adaptive_avg_pool2d):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "        self.backbone = EfficientNet.from_pretrained(model_name)\n",
    "        in_features = getattr(self.backbone,'_fc').in_features\n",
    "        self.classifier = nn.Linear(in_features,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        features = self.pool_type(self.backbone.extract_features(x),1)\n",
    "        features = features.view(x.size(0),-1)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "class MedathonDenseNet(nn.Module):\n",
    "    def __init__(self,model_name='densenet121',pool_type=F.adaptive_avg_pool2d):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "        self.backbone = DenseNet.from_pretrained(model_name)\n",
    "        in_features = getattr(self.backbone,'classifier').in_features\n",
    "        self.classifier = nn.Linear(in_features,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        features = self.pool_type(self.backbone.extract_features(x),1)\n",
    "        features = features.view(x.size(0),-1)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "class MedathonResNet(nn.Module):\n",
    "    def __init__(self,model_name='resnet18',pool_type=F.adaptive_avg_pool2d):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "        self.backbone = ResNet.from_pretrained(model_name)\n",
    "        in_features = getattr(self.backbone,'fc').in_features\n",
    "        self.classifier = nn.Linear(in_features,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        features = self.pool_type(self.backbone.extract_features(x),1)\n",
    "        features = features.view(x.size(0),-1)\n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kfh7vc13zxi9"
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def get_model(model_name='efficientnet-b0',lr=1e-5,wd=0.01,freeze_backbone=False,opt_fn=torch.optim.AdamW,device=None):\n",
    "    device = device if device else get_device()\n",
    "    if model_name.startswith('dense'):\n",
    "        model = MedathonDenseNet(model_name=model_name)\n",
    "    elif model_name.startswith('eff'):\n",
    "        model = MedathonEfficientNet(model_name=model_name)\n",
    "    elif model_name.startswith('res'):\n",
    "        model = MedathonResNet(model_name=model_name)\n",
    "    if freeze_backbone:\n",
    "        for parameter in model.backbone.parameters():\n",
    "            parameter.requires_grad = False\n",
    "    opt = opt_fn(model.parameters(),lr=lr,weight_decay=wd)\n",
    "    model = model.to(device)\n",
    "    return model, opt\n",
    "\n",
    "def training_step(xb,yb,model,loss_fn,opt,device,scheduler):\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    out = model(xb)\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(out,yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    return loss.item()\n",
    "    \n",
    "def validation_step(xb,yb,model,loss_fn,device):\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_fn(out,yb)\n",
    "    out = torch.sigmoid(out)\n",
    "    # out = torch.softmax(out, dim=1)\n",
    "    return loss.item(),out\n",
    "\n",
    "def get_data(train_df,valid_df,train_tfms,test_tfms,bs):\n",
    "    train_ds = MedathonDataset(df=train_df,im_path=training_folder,transforms=train_tfms)\n",
    "    valid_ds = MedathonDataset(df=valid_df,im_path=valid_folder,transforms=test_tfms)\n",
    "    train_dl = DataLoader(dataset=train_ds,batch_size=bs,shuffle=True,num_workers=2)\n",
    "    valid_dl = DataLoader(dataset=valid_ds,batch_size=bs*2,shuffle=False,num_workers=2)\n",
    "    return train_dl,valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kGyYZO34pdG1"
   },
   "outputs": [],
   "source": [
    "def fit(epochs,model,train_dl,valid_dl,opt,device=None,loss_fn=loss_fn, fold=-1):\n",
    "    device = device if device else get_device()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, len(train_dl)*epochs)\n",
    "    val_rocs = [] \n",
    "    val_f1s = []\n",
    "    val_accs=[]\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    \n",
    "    #Creating progress bar\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write(['folds', 'epoch','train_loss','valid_loss','val_roc','val_F1','val_acc'],table=True)\n",
    "\n",
    "    for epoch in mb:    \n",
    "        trn_loss,val_loss = 0.0,0.0\n",
    "        val_preds = np.zeros((len(valid_dl.dataset),1))\n",
    "        val_targs = np.zeros((len(valid_dl.dataset),1))\n",
    "        \n",
    "        #Training\n",
    "        model.train()\n",
    "        \n",
    "        #For every batch \n",
    "        for xb,yb in progress_bar(train_dl,parent=mb):\n",
    "#             print(xb.permute(0,3,1,2))\n",
    "            trn_loss += training_step(xb.permute(0,3,1,2).float(),yb,model,loss_fn,opt,device,scheduler) \n",
    "        trn_loss /= mb.child.total\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i,(xb,yb) in enumerate(progress_bar(valid_dl,parent=mb)):\n",
    "                loss,out = validation_step(xb.permute(0,3,1,2).float(),yb,model,loss_fn,device)\n",
    "                val_loss += loss\n",
    "                bs = xb.shape[0]\n",
    "                val_preds[i*bs:i*bs+bs] = out.cpu().numpy()\n",
    "                val_targs[i*bs:i*bs+bs] = yb.cpu().numpy()\n",
    "\n",
    "        val_loss /= mb.child.total\n",
    "        val_roc = roc_auc_score(val_targs.reshape(-1),val_preds.reshape(-1))\n",
    "        val_f1 = f1_score(val_targs.reshape(-1),np.where(val_preds.reshape(-1)<0.5,0,1))\n",
    "        val_acc = accuracy_score(val_targs.reshape(-1), np.where(val_preds.reshape(-1)<0.5,0,1))\n",
    "\n",
    "        val_rocs.append(val_roc)\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_losses.append(val_loss)\n",
    "        train_losses.append(trn_loss)\n",
    "        \n",
    "\n",
    "        mb.write([fold, epoch,f'{trn_loss:.6f}',f'{val_loss:.6f}',f'{val_roc:.6f}',f'{val_f1:.6f}',f'{val_acc:.6f}'],table=True)\n",
    "    return model,val_rocs,val_accs, val_f1s, val_losses,train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xYid5CZwz_P6"
   },
   "outputs": [],
   "source": [
    "def create_records():\n",
    "    records = pd.DataFrame()\n",
    "    records[\"epoch\"] = range(0,epochs)\n",
    "    records[\"train_loss\"] = train_losses\n",
    "    records[\"val_loss\"] = val_losses\n",
    "    records[\"val_rocs\"] = val_rocs\n",
    "    records[\"val_accs\"] = val_rocs\n",
    "    records[\"val_f1s\"] = val_rocs\n",
    "    records[\"height\"] = height\n",
    "    records[\"width\"] = width\n",
    "    records[\"model\"] = model_name\n",
    "    records[\"loss_function\"] = loss_name\n",
    "    records[\"batch_size\"] = bs\n",
    "    records[\"fold\"] = -1\n",
    "    old_records = pd.read_csv('records1.csv')\n",
    "    new_records = pd.concat([old_records, records])\n",
    "    new_records.to_csv('records1.csv', index=False)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JtWR3XIlzxbu"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ7pmeaaoIkS"
   },
   "outputs": [],
   "source": [
    "\n",
    "models = ['densenet121', 'efficientnet-b3', 'resnet18']\n",
    "sizes = [224,256,512]\n",
    "for i, model_name in enumerate(models):\n",
    "    train_tfms,test_tfms = get_augmentations(p=0.5, height=sizes[i], width= sizes[i])\n",
    "    train_dl,valid_dl = get_data(train_df,valid_df,train_tfms,test_tfms,bs)\n",
    "    model, opt = get_model(model_name=model_name,lr=1e-4,wd=1e-4)\n",
    "    model,val_rocs, val_accs, val_f1s, val_losses, train_losses, = fit(epochs,model,train_dl,valid_dl,opt, loss_fn=loss_fn)\n",
    "    torch.save(model.state_dict(),\"{}_{}_{}_{}_{}-{}_{}_{}.pth\".format(model_name, loss_name, epochs, np.max(val_rocs), height, width, bs, -1))\n",
    "    records = create_records()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Medathon_Training_Team#3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
